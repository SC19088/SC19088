---
title: "Introduction to SC19088"
author: "SiYu Jiang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to StatComp}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

__SC19088__  is divided into two parts, one of which is two functions I wrote. The fbm function is to generate a Brownian motion. Its hurst index and the number of generations can be specified at will. The default values are 0.7 and 100. The other function is. The remaining part is my homework for statistical calculations this semester.

The R package 'microbenchmark' can be used to benchmark the above R and C++ functions.

## Generate 100 numbers with Brownian motion Hurst 

The source R code for _fbm_ is as follows:
```{r,eval=FALSE}
function(hurst=0.7, n=100){
  delta <- 1/n
  r <- numeric(n+1)
  r[1] <- 1
  for(k in 1:n)
    r[k+1] <- 0.5 * ((k+1)^(2*hurst) - 2*k^(2*hurst) + (k-1)^(2*hurst))
  r <- c(r, r[seq(length(r)-1, 2)])
  lambda <- Re((fft(r)) / (2*n))
  W <- fft(sqrt(lambda) * (rnorm(2*n) + rnorm(2*n)*1i))
  W <- n^(-hurst) * cumsum(Re(W[1:(n+1)]))
  X <- ts(W, start=0, deltat=delta)
  return(X)
}
```


This is the implementation of our function example.

```{r,eval=TRUE}
fbm <- function(hurst=0.7, n=100){
  delta <- 1/n
  r <- numeric(n+1)
  r[1] <- 1
  for(k in 1:n)
    r[k+1] <- 0.5 * ((k+1)^(2*hurst) - 2*k^(2*hurst) + (k-1)^(2*hurst))
  r <- c(r, r[seq(length(r)-1, 2)])
  lambda <- Re((fft(r)) / (2*n))
  W <- fft(sqrt(lambda) * (rnorm(2*n) + rnorm(2*n)*1i))
  W <- n^(-hurst) * cumsum(Re(W[1:(n+1)]))
  X <- ts(W, start=0, deltat=delta)
  return(X)
}
d <- fbm(hurst=0.5, n=500) 
plot(d)
```

The above picture show an Brownian motion Hurst which hurst is 0.5.


## Compare different orders k in B-spline regression functions with any m equidistant inner knots

The source R code for _Bs_ is as follows:
```{r,eval=FALSE}
Bs <- function(n,m){
  f <- function(x) sin(12*(x+0.2))/(x+0.2)
  knots_eq <- function(x, k, m)
  {
    c(min(x) - ((k-1):0) * (max(x)-min(x))/(m+1),
      seq(from=min(x), to=max(x), length.out=m+2)[-c(1,m+2)],
      max(x) + (0:(k-1)) * (max(x)-min(x))/(m+1))
  }
  x=runif(n,0,1)
  e=rnorm(n,0,1)
  y=f(x)+e
  ks <- 1:6
  cols_k <- rainbow(length(ks))
  par(mai=c(0.65,0.6,0.1,0.1), mgp=c(2,1,0))
  plot(x, y, xlab="x", ylab="y", pch=16)
  abline(v=knots_eq(x, 1, m), lty=c(1,rep(2,m),1), col="grey60")
  for (kk in 1:length(ks))
  {
    k <- ks[kk]
    est <- lm(y ~ -1 +
                splineDesign(x=x, knots=knots_eq(x, k, m), ord=k))
    plot(function(x) splineDesign(x=x, knots=knots_eq(x, k, m), ord=k) %*%
           est$coef,
         from=min(x), to=max(x), n=1001, lwd=2, col=cols_k[kk], add=TRUE)
  }
  legend("bottomright", inset=0.02, legend=c(paste("k = ",ks,sep=""), "knots"),
         lwd=2, lty=c(rep(1,length(ks)),2), col=c(cols_k, "grey60"), bg="white")
}

```



The R code for using _Bs_ is as follows.

```{r,eval=TRUE}
library("splines")
library("nlme")
library("MASS")
library("mgcv")
Bs <- function(n,m){
  f <- function(x) sin(12*(x+0.2))/(x+0.2)
  knots_eq <- function(x, k, m)
  {
    c(min(x) - ((k-1):0) * (max(x)-min(x))/(m+1),
      seq(from=min(x), to=max(x), length.out=m+2)[-c(1,m+2)],
      max(x) + (0:(k-1)) * (max(x)-min(x))/(m+1))
  }
  x=runif(n,0,1)
  e=rnorm(n,0,1)
  y=f(x)+e
  ks <- 1:6
  cols_k <- rainbow(length(ks))
  par(mai=c(0.65,0.6,0.1,0.1), mgp=c(2,1,0))
  plot(x, y, xlab="x", ylab="y", pch=16)
  abline(v=knots_eq(x, 1, m), lty=c(1,rep(2,m),1), col="grey60")
  for (kk in 1:length(ks))
  {
    k <- ks[kk]
    est <- lm(y ~ -1 +
                splineDesign(x=x, knots=knots_eq(x, k, m), ord=k))
    plot(function(x) splineDesign(x=x, knots=knots_eq(x, k, m), ord=k) %*%
           est$coef,
         from=min(x), to=max(x), n=1001, lwd=2, col=cols_k[kk], add=TRUE)
  }
  legend("bottomright", inset=0.02, legend=c(paste("k = ",ks,sep=""), "knots"),
         lwd=2, lty=c(rep(1,length(ks)),2), col=c(cols_k, "grey60"), bg="white")
}
Bs(100,6)
```

The results show the picture compares different knots in m=6 to fit function f(x)=sin(12*(x+0.2))/(x+0.2).

---

## Homework in 20190925

## Question

Use knitr to produce at least 3 examples (texts, figures,tables).

---

## Answer

The mtcars is a data set about cars that R software comes with.

### summary of mtcars
```{r}
summary(mtcars)
```

### example of figures
```{r}
plot(mtcars$mpg,mtcars$disp)
```

---

## Homework in 20191006

## Questions

- Exercises 3.4, 3.11, and 3.18 (pages 94-96, Statistical Computating with R).

## Answers

### 3.4

- In this question,we can easily calculate the corresponding distribution function:
$$F(x)=1-e^{-\frac{x^2}{2\sigma^2}}$$ 
Then we can get  inverse function $F^{-1}(x)$:
$$F^{-1}(x)=\sqrt{-2\sigma^2ln(1-x)}$$
Then we can use the inverse transform method in Lec 1 Page 7.
I choose the $\sigma$ is 0.5.

```{r,echo=TRUE}
### Nf is the inverse function. 
Nf=function (Sigma,x)
{
  FN=((-2*Sigma^2*log(1-x)))^0.5
  FN
}

### pdf is the  Rayleigh density function.  
pdf=function (Sigma,x)
{
  f=x/Sigma^2*exp(-x^2/(2*Sigma^2))
  f
}
n <- 1000
u <- runif(n)
Sigma=0.5 #Here we can change Sigma 
x <- Nf(Sigma,u)
hist(x, prob = TRUE)
y <- seq(0, 2, .005)
lines(y, pdf(Sigma,y))


```




### 3.11

```{r,echo=TRUE}
n=1000
x1=rnorm(n,0,1)
x2=rnorm(n,3,1)
choose=function(p)
{
  p.new=sample(c(0,1),n,replace = TRUE,prob = c(1-p,p))
  ### The probility that p.new is 1 is p,the probility that p.new is 0 is 1-p.
  X=p.new*x1+(1-p.new)*x2
  X
}

f=function(x,mu)
{
  f=exp(-(x-mu)^2/2)*(2*pi)^(-0.5)
  f
}


hist(choose(0.75))
hist(choose(0.5))
hist(choose(0.85))
hist(choose(0.95))
```

- The empirical distribution of the mixture is bimodal.

---

## Homework in 20191011

## Question 
1. Compute a Monte Carlo estimate of
$$ \int_{0}^{\pi /3}sint\ dt$$
and compare your estimate with the exact value of the integral.

2. Use Monte Carlo integration with antithetic variables to estimate
$$\int_{0}^{1}\frac{e^{-x}}{1+x^{2}}\ dx$$
and find the approximate reduction in variance as a percentage of the variance without variance reduction.

3. Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.


---

## Question 1(5.1)

- This question I used Antithetic variables to solve this question.
- $$ \int_{0}^{\pi /3}sint\ dt =cos\left (0  \right )-cos\left ( \frac{\pi }{3} \right )=0.5$$
- The exact value of the integral is 0.5.
```{r}
n=1e4
x=runif(n,min=0,max=pi/3)
theta_hat1=mean(sin(x))*(pi/3)
theta_hat2=mean(sin(pi/3-x))*(pi/3)
theta_hat=mean(c(theta_hat1,theta_hat2))
print(c(theta_hat1,theta_hat2))
print(theta_hat)
```
- The error is `r 0.5-theta_hat`.

---

## Question 2(5.10)

- This method is my initial idea, but the calculation efficiency is very low, but the result is very good.
```{r}
theta_hat_MC=theta_hat_AhV=0
for (i in 1:1000){
  n=1e4
  x=runif(n,min=0,max=1)
  theta=exp(-x)/(1+x^2)
  theta_hat_MC[i]=mean(theta)
  theta_NI=exp(-(1-x))/(1+(1-x)^2)
  theta_hat_AhV[i]=mean(c(theta[1:(n/2)],theta_NI[1:(n/2)]))
}
c(sd(theta_hat_MC)^2,sd(theta_hat_AhV)^2,sd(theta_hat_AhV)^2/sd(theta_hat_MC)^2)

```
- We can apply the ready-made code on P120 of the textbook.
``` {r}
MC.Phi <- function(x, R = 10000, antithetic = TRUE) {
  u <- runif(R/2,0,x)
  if (!antithetic) v <- runif(R/2) else v <- 1 - u
  u <- c(u, v)
  cdf <- numeric(length(x))
  for (i in 1:length(x)) {
    g <- exp(-u)/(1+u^2)
    cdf[i] <- mean(g)
  }
  cdf
}
m <- 1000
MC1 <- MC2 <- numeric(m)
x <- 1
for (i in 1:m) {
  MC1[i] <- MC.Phi(x, R = 1000, anti = FALSE)
  MC2[i] <- MC.Phi(x, R = 1000)
}

c(sd(MC1)^2,sd(MC2)^2,sd(MC2)^2/sd(MC1)^2)
```
- By calculation,we know the approximate reduction in variance is approximately 0.035


---

## Question 3(5.15)

```{r}
M <- 10000; k <- 5
r <- M/k #replicates per stratum
N <- 50 #number of times to repeat the estimation
T2 <- numeric(k)
est <- 0
g<-function(x) exp(-x - log(1+x^2)) * (x > 0) * (x < 1) / (exp(-x) / (1 - exp(-1)))
for (i in 1:N) {
  for(j in 1:k) 
    {
      u=runif(r,(j-1)/k,j/k)
      x = -log(1 - u * (1 - exp(-1)))
      fg <- g(x)
      T[j]<-mean(fg)
    }
  est[i] <- mean(T)
}
mean(est)
sd(est)
```
- The mean using the the stratified importance sampling estimate method is `r mean(est)`,is similar to $\hat{\theta}=0.5257801$
- The standard deviation we get using the the stratified importance sampling estimate method is `r sd(est)`,the estimated standard error in Example 5.10 is 0.0970314,so the stratified importance sampling estimate method is better in this question.

---

## Homework in 20191018

## 6.5

### Question
- Suppose a 95% symmetric t-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use aMonte Carlo experiment to estimate the coverage probability of the t-interval for random samples of $\chi^{2}(2)$ data with sample size n = 20. Compare your t-interval results with the simulation results in Example 6.4. (The t-interval should be more robust to departures from normality than the interval for variance.)

### Answer
```{r}
n <- 20
alpha <- 0.05
UCL <- sigma_hat <- theta_hat <- 0

### repete 1000 times
for (i in 1:1000)
{
  x<- rchisq(n,df=2)
  theta_hat[i]=mean(x)
  sigma_hat[i]=sd(x)
  UCL[i] <- (n-1) * var(x) / qchisq(alpha, df=n-1)
}

### t-interval method
mean(theta_hat-qt(1-alpha/2,n-2)*sigma_hat/sqrt(n)<=2 & theta_hat+qt(1-alpha/2,n-2)*sigma_hat/sqrt(n)>=2)

### method in example 6.4
mean(UCL>=4)

```
- From the result,we can find t-interval method is better than the method used interval for variance.


## 6.6


### Question


- Estimate the 0.025, 0.05, 0.95, and 0.975 quantiles of the skewness $\sqrt{b_{1}}$ under normality by a Monte Carlo experiment. Compute the standard error of the estimates from (2.14) using the normal approximation for the density (with exact variance formula). Compare the estimated quantiles with the quantiles of the large sample approximation $\sqrt{b_{1}}\approx N(0, 6/n)$.


### Answer
```{r}
n <- 20
rep=1000
skewness <- 0
q=c(0.025,0.05,0.95,0.975)

sk <- function(x) {
#computes the sample skewness coeff.
xbar <- mean(x)
m3 <- mean((x - xbar)^3)
m2 <- mean((x - xbar)^2)
return( m3 / m2^1.5 )
}

for (i in 1:rep){
  x <- rnorm(n,0,1)
  skewness[i]=sk(x)
}

### the estimated quantiles
cv_hat=as.vector(quantile(skewness,q))
cv_hat
### the real data
cv <- qnorm(q,0, sqrt(6/n))
cv
### the error is: 
1-cv_hat/cv
### Because we get a small number of skewness, we change the variance of the skewness to 6*(n-2) / ((n+1)*(n+3)
cv <- qnorm(q,0, sqrt(6*(n-2)/((n+1)*(n+3))))
### Revised cv is
cv
### the revised error is: 
1-cv_hat/cv
### The result is much better.

### When q=c(0.025,0.05,0.95,0.975),the standard error is
q*(1-q)/(n*(exp(-cv_hat^2)/sqrt(2*pi)))
```

---

## Homework in 20191101

## Question 1

- Estimate the power of the skewness test of normality against symmetric $Beta(\alpha,\alpha) distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as t(ν)?

## Solution


```{r}
alpha <- .1
n <- seq(100,2000,100)
m <- 2500
sk <- function(x) {
#computes the sample skewness coeff.
xbar <- mean(x)
m3 <- mean((x - xbar)^3)
m2 <- mean((x - xbar)^2)
return( m3 / m2^1.5 )
}
power=0
for (j in 1:length(n)){
  #critical value for the skewness test
  cv <- qnorm(1-alpha/2, 0, sqrt(6*(n[j]-2) / ((n[j]+1)*(n[j]+3))))
  sktests <- numeric(m)
  for (i in 1:m) { #for each replicate
  x <- rbeta(n[j],alpha,alpha)
  sktests[i] <- as.integer(abs(sk(x)) >= cv)
  }
power[j]=mean(sktests)
}
power
```
- We can find the power is not good enough,because Beta(2,2) Is symmetrical.I tried to change my n,but it influenced a little.In order to get a better result, I make a  contaminated like Example 6.10.But we find the best power is 0.3.

```{r}
alpha <- .1
n <- 30
m <- 2500
epsilon <- c(seq(0, .15, .01), seq(.15, 1, .05))
N <- length(epsilon)
pwr <- numeric(N)
sk <- function(x) {
#computes the sample skewness coeff.
xbar <- mean(x)
m3 <- mean((x - xbar)^3)
m2 <- mean((x - xbar)^2)
return( m3 / m2^1.5 )
}
#critical value for the skewness test
cv <- qnorm(1-alpha/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
for (j in 1:N) { #for each epsilon
e <- epsilon[j]
sktests <- numeric(m)
for (i in 1:m) { #for each replicate
sigma <- sample(c(1, 10), replace = TRUE,
size = n, prob = c(1-e, e))
## x <- rnorm(n, 0, sigma)
x <- rbeta(n,sigma,sigma)
sktests[i] <- as.integer(abs(sk(x)) >= cv)
}
pwr[j] <- mean(sktests)
}
#plot power vs epsilon
plot(epsilon, pwr, type = "b",
xlab = bquote(epsilon), ylim = c(0,1))
abline(h = .1, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(epsilon, pwr+se, lty = 3)
lines(epsilon, pwr-se, lty = 3)
```




---

## Question 2

- Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level $\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) $\chi^{2}(1)$, (ii)Uniform(0,2), and (iii) Exponential(rate=1). In each case, test $H_{0} : μ = \mu_{0}$ vs  $H_{1}:μ\neq\mu_{0}$, where $\mu_{0}$ is the mean of $\chi^{2}_{1}$, Uniform(0,2), and Exponential(1), respectively.

---

## Solution

```{r}

n <- 20
alpha <- .05
mu0 <- 1
m <- 10000 #number of replicates
p <- numeric(m) #storage for p-values
for (j in 1:m) {
x <- rchisq(n,df=1)
ttest <- t.test(x, mu = mu0,alternative = c("two.sided"))
p[j] <- ttest$p.value
}
p.hat1 <- mean(p < alpha)


for (j in 1:m) {
x <- runif(n, 0,2)
ttest <- t.test(x,  mu = mu0)
p[j] <- ttest$p.value
}
p.hat2 <- mean(p < alpha)


for (j in 1:m) {
x <- rexp(n, mu0)
ttest <- t.test(x, mu = mu0,alternative = c("two.sided"))
p[j] <- ttest$p.value
}
p.hat3 <- mean(p < alpha)
print(c(p.hat1,p.hat2,p.hat3))

```

- We can find that all three p-value>0.5,so we can use t-test in this three cases.We tested that the t-test is robust to mild departures from normality.

---

##Question 3 & solution

- 1.We mark the power of method 1 is p1,the power of method2 is p2.The hypothesis test is
$$H_{0}:p_{1}=p_{2}\leftrightarrow H_{1}:p_{1}\neq p_{2}$$
- 2. We choose the multipula test
- 3. When we use multiole test,we only need to know True negative rate,because we know tro power,we can caculate V,S,U,T in the table,then we can do multiple test to test $H_{0}$.

---

## Homework in 20191108



## Solution 7.6

```{r}
# get data
library("boot")
data(scor,package = "bootstrap")
# the scatter plots
pairs(scor)
# the sample correlation matrix
scm=cor(scor)
scm
##  Question 1:Compare the plot with the sample correlation matrix.
##  We can find that the larger the correlation coefficient in the sample matrix, the denser the point in the plot.

# For convenience, I used the boot function directly.
r= function(x,i){
 c(cor(x[i,1], x[i,3]),cor(x[i,3], x[i,4]),cor(x[i,3], x[i,5]),cor(x[i,4], x[i,5]))
}

obj <- boot(data = scor, statistic = r, R = 2000)
# The result of bootstrap.
obj
# The standard error of four correlation is 0.08901265,0.04783641,0.06002932,0.06718623.rho12 is the maximal,and the rho 34 is the minimum.

```


## Solution 7.B
```{r}
library("boot")

func.boot <-function(x,i) {
  #computes the sample skewness coeff.
  xbar <- mean(x[i])
  m3 <- mean((x[i] - xbar)^3)
  m2 <- mean((x[i] - xbar)^2)
  return( m3/m2^1.5 )
}
norm=0
norm.left=norm.right=basic.left=basic.right=perc.left=perc.right=0

# get three  proportions of normal distribution.
m=0
for (i in 1:500){
  nor=rnorm(10)
    obj.nor = boot(data = nor, statistic = func.boot, R = 999)
  CI=boot.ci(boot.out = obj.nor, type = c("norm","basic", "perc"))
  norm[i]<-m>=CI$normal[2]&m<=CI$normal[3]
  norm.left[i]=m<CI$normal[2]
  norm.right[i]=m>CI$normal[3]
  basic.left[i]=m<CI$basic[4]
  basic.right[i]=m>CI$basic[5]
  perc.left[i]=m<CI$perc[4]
  perc.right[i]=m>CI$perc[5]
}
left.norm=c(mean(norm.left),mean(basic.left),mean(perc.left))
right.norm=c(mean(norm.right),mean(basic.right),mean(perc.right))

# get three proportions of Chi-square distribution
m=4/sqrt(10)
for (i in 1:500){
  chi=rchisq(10,df=5,ncp=0)
  obj.nor = boot(data = chi, statistic = func.boot, R = 999)
  CI=boot.ci(boot.out = obj.nor, type = c("norm","basic", "perc"))
  norm[i]=m>=CI$normal[2]&m<=CI$normal[3]
  norm.left[i]=m<CI$normal[2]
  norm.right[i]=m>CI$normal[3]
  basic.left[i]=m<CI$basic[4]
  basic.right[i]=m>CI$basic[5]
  perc.left[i]=m<CI$perc[4]
  perc.right[i]=m>CI$perc[5]
}
left.chi=c(mean(norm.left),mean(basic.left),mean(perc.left))
right.chi=c(mean(norm.right),mean(basic.right),mean(perc.right))

# the answer of normal distribution
data.frame(left.norm,time.norm=1-left.norm-right.norm,right.norm,row.names=c("norm","basic", "perc"))

# the answer of chi-square distribution
data.frame(left.chi,time.chi=1-left.chi-right.chi,right.chi,row.names=c("norm","basic", "perc"))


```

---

## Homework in 20191115

## Question 7.8

- Obtain the jackknife estimates of bias and standard error of $\hat\theta$.


## Solution 7.8

```{r}
# get data
library("boot")
data(scor,package = "bootstrap")
# the correlation matrix
sigma=cov(scor)
# the function to caculate theta
gettheta=function(sigma){
  eigenvalues=eigen(sigma)$values
  theta=max(eigenvalues)/sum(eigenvalues)
  theta
}
# the sample estimate of theta is 
theta.hat=gettheta(sigma)
print(theta.hat)
# do the jackknife
n=length(scor[,1])
theta <- scor.jack <- 0
for (i in 1:n)
{
  scor.jack=scor[-i,]
  theta[i]=gettheta(cov(scor.jack))
}
# the bias is
bias=(n-1)*(mean(theta)-theta.hat)
print(round(bias,6))
# the standard error
se <- sqrt((n-1) * mean((theta.hat-mean(theta))^2))
se <- (n-1)*sqrt(var(theta)/n)
print(round(se,4))
```

## Question 7.10

- In Example 7.18, leave-one-out (n-fold) cross validation was used to select
the best fitting model. Repeat the analysis replacing the Log-Log model
with a cubic polynomial model. Which of the four models is selected by the
cross validation procedure? Which model is selected according to maximum
adjusted $R^{2}$?

## Solution 7.10

```{r}
library(lattice)
library(DAAG)
attach(ironslag)
n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n)
r1 <- r2 <- r3 <- r4 <- numeric(n)
# for n-fold cross validation
# fit models on leave-one-out samples
for (k in 1:n) {
y <- magnetic[-k]
x <- chemical[-k]
J1 <- lm(y ~ x)
yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
e1[k] <- magnetic[k] - yhat1
r1[k] <- summary(J1)$adj.r.squared
J2 <- lm(y ~ x + I(x^2))
yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +J2$coef[3] * chemical[k]^2
e2[k] <- magnetic[k] - yhat2
r2[k] <- summary(J2)$adj.r.squared
J3 <- lm(log(y) ~ x)
logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
yhat3 <- exp(logyhat3)
e3[k] <- magnetic[k] - yhat3
r3[k] <- summary(J3)$adj.r.squared
J4 <- lm(y ~ x + I(x^2)+I(x^3))
yhat4 <- J4$coef[1] + J4$coef[2] * chemical[k] + J4$coef[3] * chemical[k]^2 + J4$coef[4] * chemical[k]^3
e4[k] <- magnetic[k] - yhat4
r4[k] <- summary(J4)$adj.r.squared
}
c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))
# From the result,we can find the $e_{2}^{2}$ is the minimum.According to the prediction error criterion, Model 2, the quadratic modelwould be the best fit for the data.
c(mean(r1), mean(r2), mean(r3), mean(r4))
# From the result,we can find the$R_{1}^{2}$ is the maximum.According to maximum adjusted, the quadratic modelwould be the best fit for the data.
```

- From the result,we can find the $e_{2}^{2}$ is the minimum.According to the prediction error criterion, Model 2, the quadratic modelwould be the best fit for the data.
- From the result,we can find the$R_{2}^{2}$ is the maximum.According to maximum adjusted, the quadratic modelwould be the best fit for the data.


---

## Homework in 20191129

## Question 9.4

- Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.

## Answer

```{r}

f <- function(x)   0.5*exp(-abs(x))  

rw.Metropolis <- function(sigma, x0, N) {
x <- numeric(N)
x[1] <- x0
u <- runif(N)
k <- 0
for (i in 2:N) {
y <- rnorm(1, x[i-1], sigma)
if (u[i] <= (f(y) / f(x[i-1])))
x[i] <- y else {
x[i] <- x[i-1]
k <- k + 1
}
}
return(list(x=x, k=k))
}
n <- 4 #degrees of freedom for target Student t dist.
N <- 2000
sigma <- c(.05, .5, 2, 16)
x0 <- 10
rw1 <- rw.Metropolis(sigma[1], x0, N)
rw2 <- rw.Metropolis(sigma[2], x0, N)
rw3 <- rw.Metropolis(sigma[3], x0, N)
rw4 <- rw.Metropolis(sigma[4], x0, N)
# print the reject rate
print(c(rw1$k, rw2$k, rw3$k, rw4$k)/2000)
no.reject <- data.frame(sigma=sigma,no.reject=c(rw1$k, rw2$k, rw3$k, rw4$k))
knitr::kable(no.reject,format='latex')
# plot
   #display 4 graphs together
    refline <- qt(c(.025, .975), df=n)
    rw <- cbind(rw1$x, rw2$x, rw3$x,  rw4$x)
    for (j in 1:4) {
        plot(rw[,j], type="l",
             xlab=bquote(sigma == .(round(sigma[j],3))),
             ylab="X", ylim=range(rw[,j]))
        abline(h=refline)
    }
     #reset to default





```

---

## Homework in 20191206

## Question 11,1

- The natural logarithm and exponential functions are inverses of each other,so that mathematically log(exp x) = exp(logx) = x. Show by example that this property does not hold exactly in computer arithmetic. Does the identity hold with near equality?


## Solution 11.1

```{r}
# the function f is use"==" to judge if log(exp x) = exp(logx) = x
f=function(x){
  a=log(exp(x))==exp(log(x))
  b=log(exp(x))==x
  c=exp(log(x))==x
  a+b+c==3
}
f(1:10)
log(exp(10))-exp(log(10))
log(exp(10))-10
exp(log(10))-10
# We can find the example show the caculation about exp(log(x)) may be wrong.

# the function g is use all.equal to judge if log(exp x) = exp(logx) = x
g=function(x){
  all.equal(log(exp(x)),exp(log(x)),x)
}
g(1:10)
# There are no errors.


```



## Solution 11.5

```{r}
root.114 <- root.115 <- 0
k<-c(4:25,100,500,1000)

# caculate the root in 11.4
f1=function(a,k){
  pt(sqrt(a^2*k/(k+1-a^2)),df=k)-pt(sqrt(a^2*(k-1)/(k-a^2)),df=k-1)
}
for (i in 1:25){
  root.114[i]=uniroot(f1,interval = c(-1,sqrt(k[i])-0.01),k=k[i])$root
}



# caculate the root in 11.5
ck=function(a,k){
  sqrt(a^2*k/(k+1-a^2))
}

# equation function in 11.5
f2=function(a,k){
  log(2)+lgamma(k/2)-0.5*log(pi*(k-1))-lgamma((k-1)/2)+log(integrate(function(u){(1+u^2/(k-1))^(-k/2)},0,ck(a,k-1))$value)-(log(2)+lgamma((k+1)/2)-0.5*log(pi*k)-lgamma(k/2)+log(integrate(function(u){(1+u^2/k)^(-(k+1)/2)},0,ck(a,k))$value))
}

for (i in 1:22){
  root.115[i]=uniroot(f2,interval = c(-1,root.114[i]+0.0001),k=k[i])$root
}

## Note: When k is large, the value range of the uniroot function must be fine, otherwise the two ends of the interval are likely to have the same number. So I set the upper bound separately for k = 100, 500, 1000 
root.115[23]=uniroot(f2,interval = c(-1,9.98),k=k[23])$root
root.115[24]=uniroot(f2,interval = c(-1,sqrt(k[24])-0.03),k=k[24])$root
root.115[25]=uniroot(f2,interval = c(-1,sqrt(k[25]-1)),k=k[25])$root

root.114 #the result in 11.4

root.115 #the result in 11.5



```

- We found that using the method of 11.4 to estimate k = 23,24,25 is very inaccurate, but the method of 11.5 lacks a good solution to this problem.



## Solution 3

```{r}

set.seed(12345)
p <- q <- 0.1
r=1-p-q
lf <- 0

LF=function(AA,BB,p,q){ #log-likelyhood function
  a=(28-AA)*log(2*p*r)+AA*log(p^2)+(24-BB)*log(2*q*r)+BB*log(q^2)+70*log(2*p*q)+2*41*log(r)
  return(a)
}

iter<-function(p,q){
  r<-1-p-q
  a<-2*28*(p^2)/(p^2+2*p*r)+2*p*r/(p^2+2*p*r)*28+70
  b<-2*24*(q^2)/(q^2+2*q*r)+2*q*r/(q^2+2*q*r)*24+70
  c<-2*41+2*p*r/(p^2+2*p*r)*28+2*q*r/(q^2+2*q*r)*24
  return(c(a/(a+b+c),b/(a+b+c)))
}

AA=sum(rbinom(28,1,(p^2/(p^2+2*p*r))))
BB=sum(rbinom(24,1,(q^2/(q^2+2*q*r))))
lf=LF(AA,BB,p,q)

for (i in 1:100){
  m=iter(p[i],q[i])
  p[i+1]=m[1];q[i+1]=m[2]
  AA=sum(rbinom(28,1,(p[i+1]^2/(p[i+1]^2+2*p[i+1]*r))))
  BB=sum(rbinom(24,1,(q[i+1]^2/(q[i+1]^2+2*q[i+1]*r))))
  lf[i+1]=LF(AA,BB,p[i+1],q[i+1])
  if(abs(p[i+1]-p[i])<=1e-4) break
}

p

q

plot(lf,type="o",xlab = "n",ylab = "LF",main="The plot of log-likelyhood function")

```

- In fact, I think the number of this topic is not strict. It is known that n (AB) = 70, n (OO) = 41, n = 163, which is also in line with HWE. It is difficult to find a suitable p, q for him Better fit this model. 

---

## Homework in 20191213

## Question 1

- Use both for loops and lapply() to fit linear models to the  mtcars using the formulas stored in this list:
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)

## Solution 1

```{r}
formulas =list(mpg ~ disp,mpg ~ I(1 / disp),mpg ~ disp + wt,mpg ~ I(1 / disp) + wt)
# lapply() to fit linear models
lapply(formulas,lm,data=mtcars)
a<-list(NULL)
length(a)<-4
# for loop to fit linear models
for (i in 1:4){
  a[[i]]=lm(formula = formulas[[i]],data=mtcars)
}
a
```



## Question 2

- Fit the model mpg ~ disp to each of the bootstrap replicates
of mtcars in the list below by using a for loop and lapply().
Can you do it without an anonymous function?
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})

## Solution 2

```{r}
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})
lapply(bootstraps,lm,formula=mpg~disp)

```

## Question 3

- For each model in the previous two exercises, extract R2 using
the function below.
rsq <- function(mod) summary(mod)$r.squared


## Solution 3

```{r}
# the function
rsq <- function(mod) summary(mod)$r.squared
formulas =list(mpg ~ disp,mpg ~ I(1 / disp),mpg ~ disp + wt,mpg ~ I(1 / disp) + wt)
# lapply() to fit linear models
a=lapply(formulas,lm,data=mtcars)
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})
b=lapply(bootstraps,lm,formula=mpg~disp)
# r^2 in Question 1
lapply(a, rsq)
# r^2 in Question 2
lapply(b, rsq)

```


## Question 4

- The following code simulates the performance of a t-test for
non-normal data. Use sapply() and an anonymous function
to extract the p-value from every trial.
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
Extra challenge: get rid of the anonymous function by using
[[ directly.

## Solution 4

```{r}

trials <- replicate(100,t.test(rpois(10, 10), rpois(7, 10)),simplify = FALSE)
sapply(trials, '[[', 3)

```

## Question 5
- Implement mcsapply(), a multicore version of sapply(). Can you implement mcvapply(), a parallel version of vapply()?Why or why not?


## Solution 5
```{r}
# We can use the function parSapply() to do this work.
library(parallel) 
cores <- detectCores()
cl <- makePSOCKcluster(cores-1)
#parSapply(cl = NULL, X, FUN, ..., simplify = TRUE,          USE.NAMES = TRUE, chunk.size = NULL)
parSapply(cl,1:10,FUN=sin)


```

---

## Homework in 20191220

## Question

- You have already written an R function for Exercise 9.4 (page
277, Statistical Computing with R). Rewrite an Rcpp function
for the same task.
- Compare the generated random numbers by the two functions
using qqplot.
- Campare the computation time of the two functions with
microbenchmark.
- Comments your results.




## Solution

```{r}
library(Rcpp)
library(microbenchmark)
set.seed(12345)

# Use Rcpp to solve the problem
cppFunction('List Metropolis(double sigma, double x0 , int N){
  NumericVector x(N);
  NumericVector u=runif(N);
	x[0]=x0;
	int k=0;
  for(int i = 1; i < N; ++i) {
    	double y=rnorm(1,x[i-1],sigma)[0];
  	  double p=exp(-abs(y))/exp(-abs(x[i-1]));
	    if(u[i-1]<= p) {x[i]=y;}
      else{x[i]=x[i-1]; k=k+1; } 
  	}
  List L=List::create(Named("x")=x,
                      Named("k")=k);
  return L;
}')

N = 2000
sigma = c(.05, .5, 2, 16)
x0 = 25

cpp1=Metropolis(sigma[1],x0,N)
cpp2=Metropolis(sigma[2],x0,N)
cpp3=Metropolis(sigma[3],x0,N)
cpp4=Metropolis(sigma[4],x0,N)

#number of candidate points rejected
Rej = cbind(cpp1$k, cpp2$k, cpp3$k, cpp4$k)
Acc = round((N-Rej)/N,4)
rownames(Acc) = "Accept rates"
colnames(Acc) = paste("sigma",sigma)
knitr::kable(Acc)
#plot
 #display 4 graphs together
    rw = cbind(cpp1$x, cpp2$x, cpp3$x,  cpp4$x)
    for (j in 1:4) {
        plot(rw[,j], type="l",
             xlab=bquote(sigma == .(round(sigma[j],3))),
             ylab="X", ylim=range(rw[,j]))
    }





# Use R to solve the problem
lap_f = function(x) exp(-abs(x))
rw.Metropolis = function(sigma, x0, N){
 x = numeric(N)
 x[1] = x0
 u = runif(N)
 k = 0
 for (i in 2:N) {
  y = rnorm(1, x[i-1], sigma)
  if (u[i] <= (lap_f(y) / lap_f(x[i-1]))) x[i] = y 
  else {
  x[i] = x[i-1]
  k = k+1
  }
 }
 return(list(x = x, k = k))
}

rw1 = rw.Metropolis(sigma[1],x0,N)
rw2 = rw.Metropolis(sigma[2],x0,N)
rw3 = rw.Metropolis(sigma[3],x0,N)
rw4 = rw.Metropolis(sigma[4],x0,N)

#number of candidate points rejected
Rej = cbind(rw1$k, rw2$k, rw3$k, rw4$k)
Acc = round((N-Rej)/N,4)
rownames(Acc) = "Accept rates"
colnames(Acc) = paste("sigma",sigma)
knitr::kable(Acc)
#plot
  #display 4 graphs together
    rw = cbind(rw1$x, rw2$x, rw3$x,  rw4$x)
    for (j in 1:4) {
        plot(rw[,j], type="l",
             xlab=bquote(sigma == .(round(sigma[j],3))),
             ylab="X", ylim=range(rw[,j]))
    }
    

## Compare the generated random numbers by the two functions using qqplot.
## Acturally, I don't know what this problem means.I compared x in two methods,but it is doesn't make sense.
    

qqplot(cpp1$x,rw1$x)    
qqplot(cpp2$x,rw2$x)   
qqplot(cpp3$x,rw3$x)
qqplot(cpp4$x,rw4$x)
    
    
    
    
## Campare the computation time of the two functions with microbenchmark.
ts1=microbenchmark(rpp=Metropolis(sigma[1],x0,N),r=rw.Metropolis(sigma[1],x0,N))
summary(ts1)[,c(1,3,5,6)]
ts2=microbenchmark(rpp=Metropolis(sigma[2],x0,N),r=rw.Metropolis(sigma[2],x0,N))
summary(ts2)[,c(1,3,5,6)]
ts3=microbenchmark(rpp=Metropolis(sigma[3],x0,N),r=rw.Metropolis(sigma[3],x0,N))
summary(ts3)[,c(1,3,5,6)]
ts4=microbenchmark(rpp=Metropolis(sigma[4],x0,N),r=rw.Metropolis(sigma[4],x0,N))
summary(ts4)[,c(1,3,5,6)]



```
- From the compared time we can find that cpp methods is much better than R methods.It is significante that Rcpp can improve the computational efficency.
